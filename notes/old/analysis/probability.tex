
\break

\begin{flushleft}
Probability. 
\end{flushleft}

\begin{flushleft}
\addvspace{5pt} \hrule
\end{flushleft}	



\begin{flushleft}
\underline{Main idea:}
\end{flushleft}


$ (\Om, \F, P ), \;  P(\Om)=1 $ a probability space.\\


If picking $n$ points, $\{ \om_{n,k}\}_{k=1}^n$ ``at random'' from $\Om$, so all $ \om_{n,k} \in \Om$, then the following will be true

$$
	\lim_{n \rarw \infty } \frac{ \#\{ k \in \{ 1,2,...,n\} ; \om_{n,k} \in E \} }{n} = P(E), \; \textrm{for all} \; E \in \F,
$$

where $\#$ is the counting measure. \\

\begin{flushleft}
\underline{Nonsense:}
\end{flushleft}

 $\alpha:  \nats \rarw \Om$, $n \in \nats$, onto, but not one to one. Define $\#_n = \frac{1}{n} \# $, $N = \{ 1,2,...,n \}$. Then $(N, \pset{N}, \#_n )$ is a porobability space. Let $\alpha_n = \alpha |_N$, then this is an $\Om$ valued random varaible. Now we can define

$$
	P(E) = \lim_{n \rarw \infty} \#_n \alpha_n^{-1} (E), \; \textrm{for all} \; E \in \F
$$




\begin{flushleft}
\underline{Random varaibles:}
\end{flushleft}

$(S, \cS)$ a measurable space, $X: \Om \rarw S $ is called a random variable when it is $(\F, \cS)$-measurable. \\

Define $P_X: \cS \rarw [0,+\infty]$ by $P_X(E) = P( \{ \om \in \Om; X(\om) \in E \})$, this is the image measure by $X$.

$$
	P_X(S) = P( \{  \om \in \Om; X(\om) \in S \} ) = P( \Om ) = 1
$$

So the image measure induced by a random variable is a probability measure on its state space. \\

$P_X$ is called the dirstribution of $X$. \\

Define $F_X : \reals \rarw [0,1] = x \mapsto P_X((-\infty,x])$, this is called the cumulative distribution function.\\

From wikipedia:

``The probability density function of a random variable is the Radonâ€“Nikodym derivative of the induced measure with respect to some base measure (usually the Lebesgue measure for continuous random variables).''

ADD many details here



\begin{flushleft}
\underline{Expectation:}
\end{flushleft}

Define the expectation value of $X$ as $E(X) = \int_{\Om} X \, dP$, the integral of $X$.\\

Suppose $X$ is a simple function, then $X(\om) = \sum_{k=1}^n \, c_k \chi_{_{E_k}} (\om)$, $c_k \in S$ , unique, and $E_k \in \F$ disjoint.\\

$$
	E(X) =  \sum_{k=1}^n \, c_k P(E_k)
$$

\begin{flushleft}
\underline{Discrete rv:}
\end{flushleft}

A discrete random variable $X$ is one whose state space is countable. In this case there is a bijective map, $\gamma : S \rarw \nats$, and clearly the function $\gamma \circ X	$ is $( \F, \pset{\nats} )$-measurable. \\ We may write $S = \{ x_k := \gamma^{-1}(k) \}_{k=1}^\infty$, and may define $E_k :	= X^{-1}(x_k)$, $X(\omega) = \sum_{k=1}^\infty \, x_k \chi_{_{E_k}} $. \\ If we temporarily adopt the notation ``$p(x_k) = P(X = x_k)$''$ := P(E_k)$, then


$$
	E(X) =  \sum_{k \in \nats} \, x_k p(x_k)
$$

In this simple case $\Om$ may not really be nescesary, as $( \{x_k \}, \pset{\{x_k\}}, p)$ is a probability space in it's own right, and note, with $\beta_n := X \circ \alpha_n$, $\alpha_n$ as in the above nonsense,

$$
	p(x_k) = \lim_{n \rarw \infty} \#_n \beta_n^{-1} ( x_k ), \; \textrm{for all} \; x_k
$$

%So that we can asses the random measure of a random value by making sufficiently many observations and applying the normalized counting measure.








%---------------------------------------




\section*{ Conditional Expectation }

$(X, \A, \mu)$ a $\sigma$-finite measure space, and $(X, \F)$ a measurable space, $\F \subset \A$. For any $f: X \rarw \reals$, $\A$-measurable, $h:X \rarw \reals$ is the conditional expectation of $f$ with respect to $\F$ if it is $\F$-measurable and for all $A \in \F$,

$$
\int_A f \, d\mu = \int_A h\, d\mu,
$$

\noindent we write $E[f|\F]$ for the conditional expectation of $f \wrt \F$. \\


\noindent
If in addition, $f \in L^1(X, \A, \mu)$, then $\nu \in \M(X, \A)$, where $\nu(E) \defeq \int_E f \, d\mu$. $(X, \F, \mu|_\F)$ is a measure space, $\nu|_\F \in \M(X, \F)$, and $ \nu|_\F \ll \mu|_\F$, then by the RNT, there exists a unique $h \in L^1(X, \F, \mu|_\F)$ such that $\nu(E) = \int_E h \, d\mu|_\F$.

$$
E[f|\F] = \frac{d \nu}{d\mu|_\F}, \; \where \nu(E) = \int_E f \, d\mu \; \fall E \in \A, \; f \in L^1(X, \A, \mu),\; \F \subset \A.
$$

Remarks:

\begin{enumerate}
\item
For $(X, \A, \mu)$ a measure space, $f,g: X \rarw \reals$, $\A$-measurable. Then $ \int_A f \,d\mu = \int_A g \, d\mu$ for all $A \in \A \rimply f = g$, $\mu-$a.e. Proof: let $h = f - g$, $A^+ = h^{-1}( [0,\infty] )$, $A^- = h^{-1}( [-\infty,0) )$. Then $h \ge 0$ on $A^+$ so $\int_{A^+} h \,d\mu = 0$ $\rimply h=0$ $\mu$-a.e. on $A^+$ by the vanishing principle. Similarly, $h^- = 0$ $\mu$-a.e. on $A^-$.
\item
In (1), what is really needed for the result is that the integrals of $f$ and $g$ agree on $(f-g)^{-1} (\reals^+) \in \A$ and $(f-g)^{-1} (\reals^-) \in \A$.
\item Let $h$ be the conditional expectation of $f \wrt \F$, with $F \subset \A$, $\Delta \defeq f-h$. $h$ is $\F$-measurable, so is $\A$-measurable, so $\Delta$ is too, and so $\Delta^{-1}(\reals^{+,0}) \in \A$, and $\Delta^{-1}(\reals^{-}) \in \A$. By (2), if in addition, $\Delta^{-1}(\reals^{+,0}) \in \F$, and $\Delta^{-1}(\reals^{-}) \in \F$, then $f=h$ $\mu-$a.e., so in some sense, the larger $\F$ is, the closer an approximation $h$ is of $f$.
\end{enumerate}





